# requirements.txt
# ============================================
# Architecture Hybride CPU/GPU
# ============================================
# - Audio/STT/TTS: CPU (onnxruntime CPU)
# - LLM: GPU (Ollama gère CUDA indépendamment)
# ============================================

# UI Streamlit
streamlit>=1.30.0,<2.0.0
requests==2.32.5

# Audio I/O (CPU)
sounddevice==0.5.2
soundfile==0.13.1

# STT - Faster-Whisper (CPU)
faster-whisper==1.2.0
ctranslate2>=4.6.0

# TTS - Piper (CPU via onnxruntime)
piper-tts==1.3.0
onnxruntime==1.18.1  # CPU uniquement - Ollama gère son propre CUDA

# Utils
numpy>=1.24
rapidfuzz>=3.9

# LLM Client (Ollama utilise CUDA via llama.cpp)
ollama>=0.1.30

# MCP Protocol (Model Context Protocol SDK)
mcp>=1.1.0
anyio>=4.0.0
